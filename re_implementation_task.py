# -*- coding: utf-8 -*-
"""Re-Implementation Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y_6Hnj8vazTLARgtBRFnsSIexEVrE860

# **Multimodal Emotion Recognition Using Cross-Modal Translation**

**Installing some mendatory libraries to access CMU-MOSEI dataset via SDK**
"""

pip install multimodal-sdk
!pip install h5py
!pip install pandas

"""**Installing the CMU-MultimodalSDK to access the dataset from github**"""

!pip install multimodal-sdk --upgrade
!pip install git+https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK.git

"""**Using mmsdk to downlaod dataset locally and save at the specified path /content/cmumosei**

Downloading high level features which include visual, audio and textual features
"""

!pip install mmsdk
from mmsdk import mmdatasdk

dataset = mmdatasdk.mmdataset(mmdatasdk.cmu_mosei.highlevel,'cmumosei/')

"""**Read avaiable modalities in downloaded dataset**"""

print("Available modalities:", dataset.keys())

"""**Align glove_vectors**

Using average function for intervals and features to summarize the other modalities based on a same set of function

**Issues faced:** Due to weak internet connection, and low cmputation power of system, after some time system crashed when align the glove vectors with specifiec everage function of interval and features

Without fucntion the system also crashed as i tried in both ways
"""

#import numpy
#def myavg(intervals,features):
#        return numpy.average(features,axis=0)

dataset.align('glove_vectors') #,collapse_functions=[myavg])

"""**Add compututional squences for labels in the dataset**

Align all computational sequences according to the labels of a dataset. First, we fetch the opinion segment labels (All Labels) computational sequence for CMU-MOSEI
"""

dataset.add_computational_sequences(mmdatasdk.cmu_mosei.labels,'cmumosei/')

"""After aligning computational sequences align all the labels. Since every video has multiple segments according to annotations and timing label in each video"""

dataset.align('All Labels')

"""Import all the nessecary libraries that will be used for the following modules:



1.   Dataset preprocessing
2.   Feature extraction
3.   VAEGAN Translator
4.   Multimodal Transformer
5.   Model Training and Evaluation

"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import classification_report

"""# **Dataset Preparation**

Defines a custom PyTorch dataset class (EmotionDataset) and uses it with a DataLoader which extracted the following features:

**audio_features:** Array of audio features (e.g., extracted using COVAREP).

**visual_features:** Array of visual features (e.g., extracted using OpenFace).

**text_features:** Array of text features (e.g., extracted using GloVe embeddings).

**labels:** Array of labels corresponding to the emotion class of each sample.

Converts features (audio, visual, text) and label for the given index into PyTorch tensors



**FacedError: ** Dataset features are not properlly aligned with computational sequence due to incomplete processing, later on used in Dataloader in EmotionDataset class
"""

class EmotionDataset(Dataset):
    def __init__(self, audio_features, visual_features, text_features, labels):
        self.audio = audio_features
        self.visual = visual_features
        self.text = text_features
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "audio": torch.tensor(self.audio[idx], dtype=torch.float32),
            "visual": torch.tensor(self.visual[idx], dtype=torch.float32),
            "text": torch.tensor(self.text[idx], dtype=torch.float32),
            "label": torch.tensor(self.labels[idx], dtype=torch.long),
        }

# Example data loading
# Replace with the actual paths to your feature files
audio_features = dataset['COVAREP'].data  # Access COVAREP features
visual_features = dataset['OpenFace'].data  # Access OpenFace features
text_features = dataset['glove_vectors'].data  # Access GloVe vectors
labels = dataset['Opinion Segment Labels'].data# Update this with the path to your labels file.

dataset = EmotionDataset(audio_features, visual_features, text_features, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

"""# **Feature Extraction**

Two separate sequential models are used for processing audio and visual inputs. There are hidden layer like Linear and ReLU Activation are used for both features.


Audio and visual features are processed independently.


Both audio and visual features are transformed into embeddings of size 300, making it easier to combine or compare them in later stages
"""

import torch.nn as nn

class FeatureExtractor(nn.Module):
    def __init__(self):
        super(FeatureExtractor, self).__init__()
        self.audio_fc = nn.Sequential(
            nn.Linear(40, 128),
            nn.ReLU(),
            nn.Linear(128, 300)
        )
        self.visual_fc = nn.Sequential(
            nn.Linear(512, 300),
            nn.ReLU(),
            nn.Linear(300, 300)
        )

    def forward(self, audio, visual):
        audio_features = self.audio_fc(audio)
        visual_features = self.visual_fc(visual)
        return audio_features, visual_features

"""The VAEGAN class, combining encoding, decoding, and discriminative capabilities for all this softmax activation is used.

This setup is useful in generating high-quality data and learning meaningful latent representations.

Method initializes three main components of the model: an encoder, a decoder, and a discriminator.
"""

class VAEGAN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(VAEGAN, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.Softmax(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Softmax(),
            nn.Linear(hidden_dim, output_dim),
            nn.Tanh()
        )
        self.discriminator = nn.Sequential(
            nn.Linear(output_dim, hidden_dim),
            nn.Softmax(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        reconstructed = self.decoder(encoded)
        validity = self.discriminator(reconstructed)
        return reconstructed, validity

"""# **Multimoder Transformer**

Multimodal classification model using Transformers for processing audio, visual, and text data. Some of the main components used in this classifier are:

**Separate Transformer** Encoder modules are used for processing each modality: audio, visual, and text.

**Fusion Layer** combines the outputs from all modalities and performs classification.
A fully connected (FC) layer sequence is used for fusion and classification

Same methods are used for the forward pass output and feature fusion and at the end all features are concatinated and used for classification
"""

class MultimodalTransformer(nn.Module):
    def __init__(self, feature_dim, hidden_dim, num_classes):
        super(MultimodalTransformer, self).__init__()
        # Per-modality transformers
        self.audio_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=4), num_layers=2
        )
        self.visual_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=4), num_layers=2
        )
        self.text_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=4), num_layers=2
        )
        # Fusion layer
        self.fc = nn.Sequential(
            nn.Linear(feature_dim * 3, hidden_dim),
            nn.Softmax(),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, audio, visual, text):
        # Transform each modality
        audio_out = self.audio_transformer(audio.unsqueeze(0)).squeeze(0)
        visual_out = self.visual_transformer(visual.unsqueeze(0)).squeeze(0)
        text_out = self.text_transformer(text.unsqueeze(0)).squeeze(0)

        # Concatenate all modalities
        combined = torch.cat((audio_out, visual_out, text_out), dim=-1)
        output = self.fc(combined)
        return output

"""# **Training and Evaluation Pipeline:**
Training and evaluation pipline for a multimodal neural network in PyTorch, leveraging audio, visual, and text inputs. The train_model function optimizes the model parameters over batches using backpropagation, while the evaluate_model function computes performance metrics, such as precision, recall, and F1-score, using sklearn.metrics.classification_report.

The use of GPU acceleration ensures efficient computation for large datasets. This modular implementation provides a robust foundation for multimodal deep learning tasks, facilitating both effective training and detailed evaluation.
"""

from sklearn.metrics import classification_report

def train_model(dataloader, model, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for batch in dataloader:
        audio = batch["audio"].to(device)
        visual = batch["visual"].to(device)
        text = batch["text"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad()
        outputs = model(audio, visual, text)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate_model(dataloader, model, device):
    model.eval()
    predictions, true_labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            audio = batch["audio"].to(device)
            visual = batch["visual"].to(device)
            text = batch["text"].to(device)
            labels = batch["label"].to(device)

            outputs = model(audio, visual, text)
            preds = torch.argmax(outputs, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    return classification_report(true_labels, predictions)

"""# **Multimodal Emotion Classification**

 This is combining three VAEGAN translators and a Multimodal Transformer. Each translator processes input features (audio, visual, text) with a latent representation of 128 dimensions, while the transformer fuses these outputs to predict one of six emotion classes.

 Both components are trained jointly using the Adam optimizer and CrossEntropyLoss, ensuring synchronized learning of feature translation and classification.

 The training loop runs for 10 epochs, tracking the average loss per epoch, followed by evaluation using a classification report to assess precision, recall, F1-score, and accuracy. This modular and scalable design allows for seamless adaptation to other multimodal tasks.


** FacedError:** Error faced because of dataloader which is not defined previously because mentioned issues
"""

# Initialize models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translators = [VAEGAN(300, 128, 300).to(device) for _ in range(3)]  # 3 translators
model = MultimodalTransformer(300, 128, 6).to(device)  # 6 emotion classes

# Optimizer and loss function
optimizer = torch.optim.Adam(
    list(model.parameters()) + [param for translator in translators for param in translator.parameters()],
    lr=1e-4
)
criterion = nn.CrossEntropyLoss()

# Training loop
epochs = 10
for epoch in range(epochs):
    train_loss = train_model(dataloader, model, translators, optimizer, criterion, device)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}")

# Evaluation
evaluate_model(dataloader, model, device)
